Restoring ancient text using deep learning: a case study on Greek epigraphy Yannis Assael1,2,*, Thea Sommerschield1,* , Jonathan Prag1 University of Oxford1, DeepMind2 Abstract and what it might have originally been. These hy- potheses are called â€œrestorationsâ€ (Bodel, 2012). Ancient History relies on disciplines such as The present work offers a fully automated aid to Epigraphy, the study of ancient inscribed texts, the epigraphistâ€™s restoration task. for evidence of the recorded past. However, Restoring text is a complex and time-consuming these texts, â€œinscriptionsâ€, are often damaged task (Woodhead, 1967; Mattingly, 1996). over the centuries, and illegible parts of the Epigraphists rely on accessing vast repositories text must be restored by specialists, known as epigraphists. This work presents PYTHIA, the of information to find textual and contextual first ancient text restoration model that recov- â€œparallelsâ€ (recurring expressions in similar ers missing characters from a damaged text documents). These repositories primarily consist input using deep neural networks. Its archi- in a researcherâ€™s mnemonic repertoire of such tecture is carefully designed to handle long- parallels, and in digital corpora for performing term context information, and deal efficiently â€œstring matchingâ€ searches (The Packard Human- with missing or corrupted character and word ities Institute, 2005; Clauss, 2012). However, representations. To train it, we wrote a non- trivial pipeline to convert PHI, the largest digi- minor differences in the search query can exclude tal corpus of ancient Greek inscriptions, to ma- or obfuscate relevant results, making it hard to chine actionable text, which we call PHI-ML. estimate the true probability distribution of pos- On PHI-ML, PYTHIAâ€™s predictions achieve sible restorations. To the best of our knowledge, a 30.1% character error rate, compared to this is the first work to bypass the constraints the 57.3% of human epigraphists. Moreover, of current epigraphic methods by means of a in 73.5% of cases the ground-truth sequence fully automated deep learning model, PYTHIA, was among the Top-20 hypotheses of PYTHIA, which aids the task of ancient text restoration. which effectively demonstrates the impact of this assistive method on the field of digital It is supplemented by PHI-ML, an epigraphic epigraphy, and sets the state-of-the-art in an- dataset of a machine actionable text. PYTHIA cient text restoration. takes as input a sequence of damaged text, and is 1 Introduction One of the key sources for Ancient History is the discipline of epigraphy, which delivers firsthand evidence for the thought, society and history of an- cient civilisations. Epigraphy is the study of doc- uments, â€œinscriptionsâ€, written on a durable sur- face (stone, ceramic, metal) by individuals, groups and institutions of the past (Davies and Wilkes, 2012). Only a small minority of surviving inscrip- tions are fully legible and complete, as many have been damaged in time (Figure 1). An epigraphist Figure 1: Damaged inscription: a decree concerning must then hypothesise how much text is missing, the Acropolis of Athens (485/4 BCE). IG I3 4B. (CC *These authors contributed equally to this work. BY-SA 3.0, WikiMedia) 9102 tcO 41 ]LC.sc[ 1v26260.0191:viXra
trained to predict character sequences comprising tion (Tracy and Papaodysseus, 2009; Panagopou- the hypothesised restorations. It works both at a los et al., 2009; Faigenbaum-Golovin et al., 2016), character- and a word-level, thereby effectively text analysis (Rao et al., 2009b,a, 2010; Ya- handling incomplete or missing words. PYTHIA dav et al., 2010; Lee and Haug, 2010; Vatri can furthermore be used by all disciplines dealing and McGillivray, 2018), and machine translation with ancient texts (philology, papyrology, codi- (PagÃ©-Perron et al., 2017; Luo et al., 2019). cology) and applies to any language (ancient or modern). To aid and encourage future research, 3 Generating PHI-ML PYTHIA and PHI-ML have been open-sourced Due to availability of digitised epigraphic corpora, at http://github.com/sommerschield/ancient-text- restoration. PYTHIA has been trained on ancient Greek (hence- forth, "AG") inscriptions, written in the ancient Greek language between 7th century BCE and 5th 2 Related work century CE. We chose AG epigraphy as a case computer vision (NLP) has dealt with study for two reasons: a) the variability of contents tasks akin to text restoration. Indeed, standard and context of the AG epigraphic record makes it count-based n-gram language models (LM) share an excellent challenge for NLP; b) several digital with epigraphists the â€œparallel-findingâ€ approach. AG textual corpora have been recently created, the N-gram models are outperformed by neural lan- largest ones being PHI (The Packard Humanities guage models, which operate at a word-level Institute, 2005; Gawlinski, 2017) for epigraphy; (Mikolov et al., 2010, 2011), at a subword- or Perseus (Smith et al., 2000) and First1KGreek character-level (Sutskever et al., 2011; Mikolov (Crane et al., 2014) for ancient literary texts. et al., 2012; Botha and Blunsom, 2014), or a com- When restoring damaged AG inscriptions, the bination of both, known as character-aware lan- epigraphistsâ€™ conjectures on the total number of guage models (Miyamoto and Cho, 2016; Kim missing characters are guided by grammatical and et al., 2016; Hwang and Sung, 2017). Despite syntactical considerations, as well as by the re- our efforts to include BERT (Devlin et al., 2018) constructed graphical layout of the inscription. in our evaluation, we found that the excessive re- Conjectured missing characters are conventionally sources required did not allow for training on a marked with hyphens, one hyphen equating to one single GPU. Text restoration also shares similar- missing character. Additionally, epigraphists tra- ities with machine reading comprehension (Her- ditionally convert edited texts to lower case and mann et al., 2015; KocË‡isky` et al., 2018), and cloze add punctuation and diacritics, which are gener- deletion tests (Hill et al., 2016; Bajgar et al., 2017; ally absent from the original inscription. These Fedus et al., 2018; Xie et al., 2018; Zhang et al., conventions were also used in PHI. 2018). Although word-level language modelling Because human annotations in PHI were noisy is capable of capturing context information more and often syntactically inconsistent (Iversen, efficiently than character-level alternatives, dam- 2007), we wrote a pipeline to convert it into a aged inscriptions preserve only limited parts of machine actionable text. We first computed the words, complicating the learning of representa- character frequencies and standardised the AG al- tions. To overcome this issue, PYTHIA works phabet to include all core characters, including simultaneously at both a character- and a word- all accentuation (147 characters), numbers, spaces level, thereby capturing long-term dependencies and punctuation marks. Two additional charac- (â€œcontext informationâ€). ters were introduced: â€˜-â€™ representing a missing Finally, several works have used machine learn- character, and â€˜?â€™ signifying a character to be pre- ing to study ancient inscriptions, focusing on as- dicted. Then we wrote regular expressions to re- sistive tools (Roued-Cunliffe, 2010), optical char- place all AG numerical notations appearing in the acter recognition and visual analysis (Terras and texts with 0 to avoid numerical correlations, strip Robertson, 2006; Garz et al., 2014; Soumya and the remaining punctuation marks, remove the con- Kumar, 2014; Shaus, 2017; Hussien et al., 2015; ventional epigraphical symbols surrounding cer- Amato et al., 2016; Can et al., 2016; Suganya tain characters (â€œLeiden Conventionsâ€), and dis- and Murugavalli, 2017; Palaniappan and Adhikari, card notes whose content was not in Greek. We 2017; Avadesh and Goyal, 2018), writer identifica- then proceeded to clear human comments, fix
the spacing and cases of duplicate punctuation, contain missing characters were mapped to â€˜unkâ€™, and filtered the resulting text so as to retain only an embedding for unknown words. Figure 2 the restricted alphabetical characters. The texts illustrates PYTHIA processing the phrase Î¼Î·Î´Î­Î½ with fewer than 100 characters were also dis- á¼„Î³Î±Î½. Finally, to allow better modelling we carded. Lastly, we matched the number of missing used a bidirectional LSTM encoder and refer to characters with those conjectured by epigraphists, this architecture as PYTHIA-BI-WORD. Further thereby converting the length value to an equal details are given in Appendix A. number of â€˜-â€™ symbols. Obtaining suggestions. To better aid the The resulting dataset is named PHI-ML, and epigraphistâ€™s task, PYTHIA returns multiple pre- consists of more than 3.2 million words (Table 1). dictions as well as the level of confidence for The inscriptions whose PHI IDs ended in {3, 4} each result, rather than a single prediction per text (every inscription in PHI was assigned a unique restoration. Specifically, we provide a set of the identifier when the original corpus was created) Top 20 predictions decoded using beam search. were held out and used respectively as test and val- idation sets. 5 Experimental evaluation Split Inscriptions Words Chars The ground-truths for incomplete epigraphic texts Train 34, 952 2, 792k 16, 300k were lost over millennia. Consequently, in or- Valid 2, 826 211k 1, 230k der to generate a ground-truth sequence, we arti- Test 2, 949 223k 1, 298k ficially removed part of the input text and treated this as the ground-truth sequence. On each train- Table 1: Statistics for the PHI-ML corpus. ing step we selected an inscription and sampled a start index and a length value âˆˆ [100, 1000], and 4 Restoring text using PYTHIA extracted the context text ğ‘¥, which was then used as input. Within ğ‘¥, we sampled a new start index PYTHIAâ€™s architecture is a sequence-to- and length âˆˆ [1, 10] to select the target sequence sequence (Sutskever et al., 2014) based neural ğ‘¦; its charactersâ€™ positions were replaced with the network architecture, consisting of a Long-Short special symbol â€˜?â€™, which denotes the positions to Term Memory (LSTM) (Hochreiter and Schmid- be predicted. The test and validation sets used huber, 1997) encoder, an LSTM decoder, and an the maximum context length. Beam search with attention mechanism introduced by Luong et al. a beam width of 100 was used to decode hypoth- (2015); Bahdanau et al. (2014). The encoder takes esis. To simplify comparisons, all AG accentua- an inscription text ğ‘¥ as input, where the symbol tion was discarded, as inputting accents was time- â€˜-â€™ denotes the missing characters, and â€˜?â€™ the consuming for the human evaluations described in blanks to be predicted. The input characters are the following paragraph. This decision did not no- first passed through a lookup table with learnable ticeably influence the reported scores. embedding vectors. Next, the encoded sequence is used as input for the decoder, which is trained 5.1 Methods evaluated to predict the content of the â€˜?â€™ characters, as shown in Figure 2. Attention allows the decoder ANCIENT HISTORIAN. Because text restoration to â€œattendâ€ to parts of the input sequence rele- is an extremely time-consuming task even for an vant to the current output, thus improving the expert epigraphist, we set out to evaluate the diffi- modelling of long-term dependencies. To further culty of the restoration task at hand - and thereby improve performance, we designed PYTHIAâ€™s judge the impact of our work - with the help of encoder to take an additional input stream of two doctoral students with epigraphical expertise. word embeddings, as it is difficult to model the The scholars were allowed to use the training set word-level context using only character-level to search for â€œparallelsâ€, and made an average of information. Thus, we generated a list of the 100k 50 restorations in 2 hours, with a 57.3% character most frequent words appearing in PHI-ML, and error rate (CER). using a separate lookup table we concatenated at LM PHILOLOGY. To evaluate the performance each time-step the embedding of each character of a model using â€œparallelsâ€, we trained a LM. with the embedding of the word it belongs to. Since large parts of the text are garbled, making Words that do not appear in the list, or that complete words unidentifiable, and because BERT
output Î³ Î± attention + input enc enc enc enc enc enc enc enc enc enc dec dec character emb. Âµ Î· Î´ Î­ Î½ á¼„ ? ? Î½ [sos] Î³ word emb. ÂµÎ·Î´Î­Î½ [unk] Figure 2: PYTHIA-BI-WORD processing the phrase Î¼Î·Î´Î­Î½ á¼„Î³Î±Î½ (meÂ¯dÃ©n Ã¡gan) â€œnothing in excessâ€, a fabled maxim inscribed on Apolloâ€™s temple in Delphi. The letters â€œÎ³Î±â€ are the characters to be predicted, and are annotated with â€˜?â€™. Since á¼„??Î½ is not a complete word, its embedding is treated as unknown (â€˜unkâ€™). The decoder outputs correctly â€œÎ³Î±â€. was not an option, the LM works at a character- Method CER Top-20 level and is based on the setup of Zaremba et al. Ancient Historian 57.3% âˆ’ (2015) (Appendix B). The LM was trained on two LM Philology 68.1% 26.0% larger digital corpora of literary AG texts (â€œphilol- LM Philology & Epigraphy 65.0% 28.8% ogyâ€), First1KGreek and Perseus, and evaluated LM Epigraphy 52.7% 47.0% on PHI-ML. PYTHIA-UNI 42.2% 60.6% LM PHILOLOGY & EPIGRAPHY. LM jointly PYTHIA-BI 32.5% 71.1% trained on First1KGreek, Perseus and PHI-ML. PYTHIA-BI-WORD 30.1% 73.5% LM EPIGRAPHY. LM trained on PHI-ML. Table 2: Predictive performance on PHI-ML. PYTHIA-UNI. An ablation architecture, using only characters as input and unidirectional LSTM. beddings as inputs, with a CER of 30.1%. Fur- PYTHIA-BI. Similar to the PYTHIA-UNI abla- thermore, the ground-truth appeared among the 20 tion, but with a bidirectional LSTM. most probable predictions of PYTHIA-BI-WORD PYTHIA-BI-WORD. This is our proposed model 73.5% of the times, which indicates that it could of choice, which uses a bidirectional LSTM and be a uniquely effective assistive tool. both characters and words as inputs. 5.3 The importance of context 5.2 Results The presence of context information is a determin- The aforementioned methods were evaluated us- ing factor in the F1-score of epigraphic restora- ing: a) the character error rate (CER) of the top tions. We therefore evaluated the impact of differ- prediction and the target sequence, b) the Top- ent textual lengths acting as augmented context on 20 F1-score score, where we ascertain whether the Top-20 F1-score measure of PYTHIA. As can the ground-truth sequence exists within the first be seen in Figure 3, the correlation between the 20 predictions. The latter evaluates the effec- â€œcontext lengthâ€ and the predictive performance tiveness of PYTHIA as an assistive tool provid- of our model is positive. Specifically, the per- ing restoration suggestions to epigraphists. As shown in Table 2, the ancient historiansâ€™ restora- 75% tions had a CER of 57.3%, which is telling of 70% the difficulty of the task. The language model trained on epigraphic datasets performed slightly 60% better, with a CER of 57.3%. Interestingly, the two attempts to use larger philological datasets performed worse. This is very likely due to a di- 50% vergence in epigraphical and literary cultures. The CER of the unidirectional PYTHIA-UNI and the 40% bidirectional PYTHIA-BI alternatives were 42.2% 10 20 50 100 200 500 1000 Context length and 32.5% respectively. The top score was there- fore achieved by the bidirectional PYTHIA-BI- WORD, which took both word and character em- 02-poT ycaruccA Figure 3: Predictive performance of the PYTHIA-BI- WORD model under different context lengths.
formance peaks around 500 characters of context importance of context in the prediction process. length. Furthermore, Figure 3 exemplifies the in- 5.5 Restoring full texts creased difficulty faced by the model when only a short context length (e.g. 20 characters) is of- We then applied PYTHIA iteratively in order to fered. The latter scenario recalls the similar diffi- predict all the missing text of an AG inscription, culties encountered by string-matching and â€œpar- comparing PYTHIAâ€™s predictions with an edition allelâ€ search approaches, where the search queries of reference (Rhodes and Osborne, 2003). In would often be short. Figure 5 the correct restorations are highlighted in colour blue and erroneous ones are in purple. 5.4 Visualising PYTHIAâ€™s attention In a real-world scenario, PYTHIA would provide We set up an example modifying lines b.8 - c.5 more than one hypothesis to the epigraphist. The of the inscription MDAI(A) 32 (1907) 428, 275 ground-truth sequence did in fact exist within the (PHI ID PH316753), to evaluate PYTHIAâ€™s recep- Top-20 hypotheses in nearly all cases, illustrating tiveness to context information and visualise the the efficacy of such technologies when paired with attention weights at each decoding step. In the human decision-making. text of Figure 4, the last word is a Greek personal á¼Ï€Î±Î¹Î½Î­ÏƒÎ±Î¹ Î´á½² á¼€Î³Î­Î»Î±Î¿Î½ Ï„á½¸Î½ á¼„ÏÏ‡Î¿Î½Ï„Î± Ï„á½¸Î½ ÏƒÏ„ÏÎ±Ï„Î·Î³á½¸Î½ Ï„á¿¶Î½ Î¸ÎµÏ„Î±Î»Î»á¿¶Î½ name ending in -Î¿Ï…. We set á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï… (â€œApol- á½…Ï„Î¹ Îµá½– ÎºÎ±á½¶Ï€ÏÎ¿Î¸ÏÂµÏ‰Ï‚ á¼Ï€Î¹ÂµÎµÎ»ÎµÏƒÎ±ÏƒÎ¸Î±Î¹ Ï€ÎµÏá½¶ á½§Î½ Î±á½Ï„Î¿á¿–Ï‚ á¼¡ Ï€ÏŒÎ»Î¹Ï‚ á¼Ï€Î·Î³Î³ lodorouâ€) as the personal name, and hid its first ÎµÎ¯Î»Î±Ï„Î¿ á¼Ï€Î±Î¹Î½Î­ÏƒÎ±Î¹ Î´á½² ÎºÎ±á½¶ Ï„á½¸Ï‚ Ï€ÏÎ­ÏƒÎ²ÎµÎ¹Ï‚ Ï„á¿¶Î½ Ï„ÎµÏ„Ï„Î±Î»á¿¶Î½ Ï„á½¸ á¼„ÏÏ‡Î¿Î½Ï„Î±Ï‚ 9 characters. This name was specifically chosen ÎºÎ±á½¶ ÎºÎ±Î»Î­ÏƒÎ±Î¹ Î±á½Ï„á½¸Ï‚ á¼Ï€á½¶ Î¾Î­Î½Î¹Î± Îµá¼°Ï‚ Ï„á½¸ Ï€ÏÏ…Ï„Î±Î½Îµá¿–Î¿Î½ Îµá¼°Ï‚ Î±á½ÏÎ¹Î¿Î½. Ï„á½´Î½ Î´á½² because it already appears within the input text. ÏƒÏ„Î®Î»Î·Î½ Ï„á½´Î½ Ï€Ïá½¸Ï‚á¼€Î»Î­Î¾Î±Î½Î´ÏÎ¿Î½ á¼€Î½Î¸ÎµÎ»Î»Ï‰Î½ Ï„á½¸Ï‚ Ï„Î±ÂµÎ¯Î±Ï‚ Ï„á¿†Ï‚ Î¸ÎµÎ¿ Ï„á¿†Ï‚ Ï€Îµ Figure 4 illustrates the attention weights for de- Ïá½¶ Ï„á½°Ï‚ ÏƒÏ…ÂµÂµÎ±Ï‡Î¯Î±Ï‚. Ï„Î¿á¿–Ï‚ Î´á½² Ï€ÏÎ­ÏƒÎ²ÎµÎ¹Ï‚ Î´Î¿Î½Î±Î¹ Ï„á½¸Î½ Ï„Î±ÂµÎ¯Î±Î½ Ï„Î¿á¿¦ Î´Î®ÂµÎ¿ Îµá¼°Ï‚ coding the first 4 missing characters. To aid visu- á¼Ï†ÏŒÎ´Î¹Î± Î´Î´ Î´ÏÎ±Ï‡Âµá½°Ï‚ á¼‘ÎºÎ¬ÏƒÏ„Ï‰Î¹. Ï„á½´Î½ Î´á½² ÏƒÏ…ÂµÂµÎ±Ï‡Î¯Î±Î½ Ï„Î®Î´Î´Îµ á¼€Î½Î±Î³ÏÎ¬ÏˆÎ±Î¹ alisation, the weights were separately scaled be- Ï„á½¸Î½ Î³ÏÎ±ÂµÂµÎ±Ï„Î­Î± Ï„á¿†Ï‚ Î²Î¿Î»á¿†Ï‚ á¼Î½ÏƒÏ„Î®Î»Î·Î¹ Î»Î¹Î¸Î¯Î½Î·Î¹ ÎºÎ±á½¶ ÏƒÏ„á¿†ÏƒÎ±Î¹ á¼Î½ á¼€ÎºÏÎ¿Ï€ ÏŒÎ»ÎµÎ¹ Îµá¼°Ï‚ Î´á½² Ï„á½´Î½ á¼€Î½Î±Î³ÏÎ±Ï†á½´Î½ Ï„á¿†Ï‚ ÏƒÏ„Î®Î»Î·Ï‚ Î´Î¿Î½Î±Î¹ Ï„á½¸Î½ Ï„Î±ÂµÎ¯Î±Î½ Ï„Î¿ Î´Î®ÂµÎ¿ tween 0 and 1 within the area of the characters 0 Î´ÏÎ±Ï‡Âµá½°Ï‚ Îµá¼¶Î½Î±Î¹ Î´á½² ÎºÎ±á½¶ Î¸ÎµÎ¹ÏŒÏ„Î·Ï„Î¿Î½ Ï„á½¸Î½ á¼ÏÏ‡Î¹Î­Î± á½¡Ï‚ Î»Î­Î³Î¿Î½Ï„Î± á¼„ÏÎ¹ÏƒÏ„Î± Îº to be predicted (â€˜?â€™) in green, and of the rest Î±á½¶ Ï€ÏÎ¬Ï„Ï„Î¿Î½Ï„Î± á½… Ï„Î¹ á¼‚Î½ Î´ÏÎ½Î·Ï„Î±Î¹ á¼€Î³Î±Î¸á½¸Î½ Ï„á¿¶Î¹Î´Î®ÂµÏ‰Î¹ Ï„á¿¶Î¹ á¼€Î¸Î·Î½Î±Î¯Ï‰Î½ ÎºÎ±á½¶ of the text in blue; the magnitude is represented Î¸ÎµÏ„Ï„Î±Î»Îµá¿–Ï‚ á¼Î½ Ï„á¿¶Î¹ Ï„ÎµÏ„Î±Î³ÂµÎ­Î½Ï‰Î¹. by the colour intensity. As can be seen, PYTHIA is attending to the contextually-relevant parts of Figure 5: Sample restoration of the inscription IG II2 116, lines 34 - 48. Restorations are in colour blue when the text: specifically, á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï…. The name is correct, purple when incorrect. correctly predicted. As a litmus test, we substi- tuted á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï… in the input text with another 6 Conclusions personal name of the same length: á¼€ÏÏ„ÎµÎ¼Î¹Î´ÏÏÎ¿Ï… (â€œArtemidorouâ€). The predicted sequence alters PYTHIA is the first ancient text restoration model accordingly to á¼€ÏÏ„ÎµÎ¼Î¹Î´ÏÏ, thereby illustrating the of its kind. Our experimental evaluation and ab- lation studies illustrate the validity of our design decisions, and illuminate the ways PYTHIA can á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï… Îµá½Î²Î¿ÎÎ´Î¿Ï‚ --ÏÎ½Î¹Î¿Ï‚ á¼€Ï„Ï„Î¯Î½Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --- assist, guide and advance the ancient historianâ€™s á¼€ Î´Î¹Î¿Î½Î¿ÏƒÎ¯Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --------------ÎµÏƒÏ„ÏÎ¬Ï„Î¿Ï… ---ÎµÏÏ‚ ------ task - and digital humanities proper. The combi- Î¹Ï„Î¿Ï…Ï„Ï‰Î½ Ï„á¿¶Î½ á¼”Î¾ á¼„Î²Î²Î¿Ï… ÎºÏÂµÎ·Ï‚ --- ?????????Î¿Ï…. nation of machine learning and epigraphy has the á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï… Îµá½Î²Î¿ÎÎ´Î¿Ï‚ --ÏÎ½Î¹Î¿Ï‚ á¼€Ï„Ï„Î¯Î½Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --- potential to impact meaningfully the study of in- Ï€ Î´Î¹Î¿Î½Î¿ÏƒÎ¯Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --------------ÎµÏƒÏ„ÏÎ¬Ï„Î¿Ï… ---ÎµÏÏ‚ ------ scribed textual cultures, both ancient and modern. Î¹Ï„Î¿Ï…Ï„Ï‰Î½ Ï„á¿¶Î½ á¼”Î¾ á¼„Î²Î²Î¿Ï… ÎºÏÂµÎ·Ï‚ --- ?????????Î¿Ï…. By open-sourcing PYTHIA, and PHI-MLâ€™s pro- á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï… Îµá½Î²Î¿ÎÎ´Î¿Ï‚ --ÏÎ½Î¹Î¿Ï‚ á¼€Ï„Ï„Î¯Î½Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --- cessing pipeline, we hope to aid future research Î¿ Î´Î¹Î¿Î½Î¿ÏƒÎ¯Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --------------ÎµÏƒÏ„ÏÎ¬Ï„Î¿Ï… ---ÎµÏÏ‚ ------ and inspire further interdisciplinary work. Î¹Ï„Î¿Ï…Ï„Ï‰Î½ Ï„á¿¶Î½ á¼”Î¾ á¼„Î²Î²Î¿Ï… ÎºÏÂµÎ·Ï‚ --- ?????????Î¿Ï…. á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏÎ¿Ï… Îµá½Î²Î¿ÎÎ´Î¿Ï‚ --ÏÎ½Î¹Î¿Ï‚ á¼€Ï„Ï„Î¯Î½Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --- Acknowledgements Î» Î´Î¹Î¿Î½Î¿ÏƒÎ¯Î¿Ï… Îµá½ÂµÎµÎ½ÎµÎ¯Î±Ï‚ --------------ÎµÏƒÏ„ÏÎ¬Ï„Î¿Ï… ---ÎµÏÏ‚ ------ We would like to thank Leah Lazar for her valu- Î¹Ï„Î¿Ï…Ï„Ï‰Î½ Ï„á¿¶Î½ á¼”Î¾ á¼„Î²Î²Î¿Ï… ÎºÏÂµÎ·Ï‚ --- ?????????Î¿Ï…. able contributions; Brendan Shillingford, Misha Figure 4: Visualisation of the attention weights for the Denil, Ã‡agË˜lar GÃ¼lÃ§ehre, and Nando de Freitas for decoding of the first 4 missing characters. The ground- the helpful comments and discussions; and the truth text ğ‘¦ = á¼€Ï€Î¿Î»Î»Î¿Î´ÏÏ appears in the input text, and Packard Humanities Institute for having made this PYTHIA attends to the relevant parts of the sequence. data digitally available.
References William Fedus, Ian J. Goodfellow, and Andrew M. Dai. 2018. Maskgan: Better text generation via filling in Giuseppe Amato, Fabrizio Falchi, and Lucia the _______. In International Conference on Learn- Vadicamo. 2016. Visual recognition of ancient ing Representations. inscriptions using random forest and fisher vector. Journal on Computing and Cultural Angelika Garz, Nicole Eichenberger, Marcus Li- Heritage, 9(4):21. wicki, and Rolf Ingold. 2014. Hisdoc 2.0: To- ward computer-assisted paleography. Manuscript Meduri Avadesh and Navneet Goyal. 2018. Optical Cultures, Natural Sciences and Technology in character recognition for sanskrit using convolution Manuscript Studies, 7. neural networks. In International Workshop on Doc- ument Analysis Systems, pages 447â€“452. Laura Gawlinski. 2017. Review: Packard hu- manities instituteâ€™s searchable greek inscrip- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- tions. https://classicalstudies.org/scs-blog/laura- gio. 2014. Neural machine translation by jointly gawlinski/review-packard-humanities-institutes- learning to align and translate. In International Con- searchable-greek-inscriptions. Accessed on ference on Learning Representations. 2019-08-26. Ondrej Bajgar, Rudolf Kadlec, and Jan Kleindienst. Karl Moritz Hermann, Tomas Kocisky, Edward 2017. Embracing data abundance: Booktest dataset Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- for reading comprehension. In International Con- leyman, and Phil Blunsom. 2015. Teaching ma- ference on Learning Representations Workshop. chines to read and comprehend. In Advances in neural information processing systems, pages 1693â€“ Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and 1701. Noam Shazeer. 2015. Scheduled sampling for se- quence prediction with recurrent neural networks. Felix Hill, Antoine Bordes, Sumit Chopra, and Jason In Advances in Neural Information Processing Sys- Weston. 2016. The goldilocks principle: Reading tems, pages 1171â€“1179. childrenâ€™s books with explicit memory representa- tions. In International Conference on Learning Rep- John Bodel. 2012. Epigraphy and the ancient historian. resentations. In Epigraphic Evidence, pages 27â€“82. Routledge. Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Jan Botha and Phil Blunsom. 2014. Compositional Long short-term memory. Neural computation, morphology for word representations and language 9(8):1735â€“1780. modelling. In International Conference on Machine Rana S Hussien, Azza A Elkhidir, and Mohamed G Learning, pages 1899â€“1907. Elnourani. 2015. Optical character recognition of arabic handwritten characters using neural network. GÃ¼lcan Can, Jean-Marc Odobez, and Daniel Gatica- In International Conference on Computing, Control, Perez. 2016. Evaluating shape representations for Networking, Electronics and Embedded Systems En- maya glyph classification. Journal on Computing gineering, pages 456â€“461. IEEE. and Cultural Heritage, 9(3):14. Kyuyeon Hwang and Wonyong Sung. 2017. Character- Manfred Clauss. 2012. Epigraphik-datenbank clauss- level language modeling with hierarchical recur- slaby. Accessed on 2019-04-24. rent neural networks. In International Conference on Acoustics, Speech and Signal Processing, pages Gregory Ralph Crane, Monica Berti, Annette GeÃŸner, 5720â€“5724. IEEE. Matthew Munson, and Tabea Selle. 2014. Open greek and latin project. http://www.dh.uni-leipzig. Paul A. Iversen. 2007. The Packard Humanities Insti- de/wo/projects/open-greek-and-latin-project/. Ac- tute - Greek epigraphy project and the revolution in cessed on 2019-04-24. Greek epigraphy. Abgadiyat, 2.1. John Davies and John Wilkes. 2012. Epigraphy and Yoon Kim, Yacine Jernite, David Sontag, and Alexan- the historical sciences. British Academy. der M Rush. 2016. Character-aware neural language models. In American Association for Artificial Intel- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and ligence. Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- Diederik P. Kingma and Jimmy Ba. 2015. Adam: A ing. arXiv preprint arXiv:1810.04805. method for stochastic optimization. In International Conference on Learning Representations. Shira Faigenbaum-Golovin, Arie Shaus, Barak Sober, David Levin, Nadav Naâ€™aman, Benjamin Sass, Eli TomÃ¡Å¡ KocË‡isky`, Jonathan Schwarz, Phil Blunsom, Turkel, Eli Piasetzky, and Israel Finkelstein. 2016. Chris Dyer, Karl Moritz Hermann, GÃ¡abor Melis, Algorithmic handwriting analysis of judahaâ€™s mil- and Edward Grefenstette. 2018. The narrativeqa itary correspondence sheds light on composition reading comprehension challenge. Transactions of biblical texts. National Academy of Sciences, of the Association of Computational Linguistics, 113(17):4664â€“4669. 6:317â€“328.
John Lee and Dag Haug. 2010. Porting an ancient Rajesh PN Rao, Nisha Yadav, Mayank N Vahia, greek and latin treebank. In International Confer- Hrishikesh Joglekar, Ronojoy Adhikari, and Ira- ence on Language Resources and Evaluation. vatham Mahadevan. 2010. Entropy, the indus script, and language. Computational Linguistics, Jiaming Luo, Yuan Cao, and Regina Barzilay. 36(4):795â€“805. 2019. Neural decipherment via minimum-cost flow: from ugaritic to linear b. arXiv preprint Peter John Rhodes and Robin Osborne. 2003. Greek arXiv:1906.06718. historical inscriptions: 404-323 BC. Oxford Uni- versity Press, USA. Thang Luong, Hieu Pham, and Christopher D Man- ning. 2015. Effective approaches to attention-based Henriette Roued-Cunliffe. 2010. Towards a decision neural machine translation. In Empirical Methods in support system for reading ancient documents. Lit- computer vision, pages 1412â€“1421. erary and linguistic computing, 25(4):365â€“379. Harold Mattingly. 1996. The Athenian empire re- Arie Shaus. 2017. Computer Vision and Machine stored: epigraphic and historical studies. Univer- Learning Methods for Analyzing First Temple Period sity of Michigan Press. Inscriptions. Ph.D. thesis, Tel Aviv University. TomÃ¡Å¡ Mikolov, Anoop Deoras, Stefan Kombrink, David Smith, Jeffrey Rydberg-Cox, and Gregory LukÃ¡Å¡ Burget, and Jan CË‡ ernocky`. 2011. Empirical Crane. 2000. The perseus project: A digital library evaluation and combination of advanced language for the humanities. Literary and Linguistic Comput- modeling techniques. In International Speech Com- ing, 15(1):15â€“25. munication Association. A. Soumya and G. Hemantha Kumar. 2014. Classifica- TomÃ¡Å¡ Mikolov, Martin KarafiÃ¡t, LukÃ¡Å¡ Burget, Jan tion of ancient epigraphs into different periods using CË‡ ernocky`, and Sanjeev Khudanpur. 2010. Recurrent random forests. In International Conference on Sig- neural network based language model. In Interna- nal and Image Processing, pages 171â€“178. IEEE. tional Speech Communication Association. Thendral Suganya and Subramaniam Murugavalli. TomÃ¡Å¡ Mikolov, Ilya Sutskever, Anoop Deoras, Hai- 2017. Feature selection for an automated ancient Son Le, Stefan Kombrink, and Jan Cernocky. 2012. tamil script classification system using machine Subword language modeling with neural networks. learning techniques. In International Conference on Preprint, 8. Algorithms, Methodology, Models and Applications in Emerging Technologies, pages 1â€“6. IEEE. Yasumasa Miyamoto and Kyunghyun Cho. 2016. Gated word-character recurrent language model. In Ilya Sutskever, James Martens, and Geoffrey E Hin- Empirical Methods in Natural Language Process- ton. 2011. Generating text with recurrent neural ing, pages 1992â€“1997. networks. In International Conference on Machine Learning, pages 1017â€“1024. Ã‰milie PagÃ©-Perron, Maria Sukhareva, Ilya Khait, and Christian Chiarcos. 2017. Machine translation and Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. automated analysis of the sumerian language. In As- Sequence to sequence learning with neural net- sociation for Computational Linguistics. works. In Advances in neural information process- ing systems, pages 3104â€“3112. Satish Palaniappan and Ronojoy Adhikari. 2017. Deep learning the indus script. arXiv preprint Melissa Terras and Paul Robertson. 2006. Image to in- arXiv:1702.00523. terpretation: an intelligent system to aid historians in reading the Vindolanda texts. Oxford University Michail Panagopoulos, Constantin Papaodysseus, Press. Panayiotis Rousopoulos, Dimitra Dafi, and Stephen Tracy. 2009. Automatic writer identification of The Packard Humanities Institute. 2005. PHI greek ancient greek inscriptions. Transactions on pat- inscriptions. https://inscriptions.packhum.org/. Ac- tern analysis and machine intelligence, 31(8):1404â€“ cessed on 2019-04-24. 1414. Stephen V. Tracy and Constantin Papaodysseus. 2009. Rajesh PN Rao, Nisha Yadav, Mayank N Vahia, The study of hands on greek inscriptions: The need Hrishikesh Joglekar, R Adhikari, and Iravatham for a digital approach. American Journal of Archae- Mahadevan. 2009a. Entropic evidence for lin- ology, 113(1):99â€“102. guistic structure in the indus script. Science, 324(5931):1165â€“1165. Alessandro Vatri and Barbara McGillivray. 2018. The diorisis ancient greek corpus. Research Data Jour- Rajesh PN Rao, Nisha Yadav, Mayank N Vahia, nal for the Humanities and Social Sciences. Hrishikesh Joglekar, R Adhikari, and Iravatham Ma- hadevan. 2009b. A markov model of the indus Arthur Woodhead. 1967. The study of Greek inscrip- script. Proceedings of the National Academy of Sci- tions, volume 424. Cambridge University Press ences, 106(33):13685â€“13690. Archive.
Qizhe Xie, Guokun Lai, Zihang Dai, and Eduard Hovy. 2018. Large-scale cloze test dataset created by teachers. In Empirical Methods in Natural Lan- guage Processing, pages 2344â€“2356. Nisha Yadav, Hrishikesh Joglekar, Rajesh PN Rao, Mayank N Vahia, Ronojoy Adhikari, and Iravatham Mahadevan. 2010. Statistical analysis of the indus script using n-grams. PLoS One, 5(3):e9506. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2015. Recurrent neural network regularization. In International Conference on Learning Representa- tions. Zhuosheng Zhang, Yafang Huang, Pengfei Zhu, and Hai Zhao. 2018. Effective character-augmented word embedding for machine reading comprehen- sion. In CCF International Conference on Natu- ral Language Processing and Chinese Computing, pages 27â€“39. Springer.
A PYTHIA training parameters Both encoder and decoder of PYTHIA consist of 2-layers with 512 hidden units. During training, we use dropout with probability 0.2 and scheduled sampling with probability 0.5 (Bengio et al., 2015). All models were trained on an 8-core machine with an NVIDIA 1080 Ti graphics processing unit (GPU). The batch size was 32 and the network weights were optimised using Adam (Kingma and Ba, 2015) with a learning rate of 10âˆ’3 and gradient clipping of 5. B LM training parameters The language modelling LSTM network consists of 2-layers with 1024 hidden units and an equally sized character embedding space. The parameters were trained using Adam with a learning rate of 2 Â· 10âˆ’3, a decay of 0.95, gradient norm clipping of 5, and dropout probability 0.2 for the inputs and the hidden layers.
