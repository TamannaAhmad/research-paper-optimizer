{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TamannaAhmad/research-paper-optimizer/blob/main/quality_assurance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "eD6wl91FTsHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_BWsGoF0Tn1D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import re\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4beBOAah5c1y",
        "outputId": "c14b5609-05f7-4eca-ab6e-13e3009ab667"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "j_NQtO5eTn1I"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MTTJoPmkTn1M"
      },
      "outputs": [],
      "source": [
        "def sanitize_text(text):\n",
        "    # remove phone numbers??? sensitive information\n",
        "    text = re.sub(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \"[REDACTED]\", text)\n",
        "    return text\n",
        "\n",
        "def get_bert_embeddings(text):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "    # get the [CLS] token embedding\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "    return embeddings[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pKUmx_0YTn1O"
      },
      "outputs": [],
      "source": [
        "def process_file(filepath):\n",
        "    try:\n",
        "        if filepath.endswith(\".txt\"):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "                return file.read()\n",
        "        elif filepath.endswith(\".pdf\"):\n",
        "            with open(filepath, \"rb\") as f:\n",
        "                pdf_reader = PyPDF2.PdfReader(f)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() or \"\"\n",
        "                return text\n",
        "        else:\n",
        "            return None  # unsupported file type\n",
        "\n",
        "    except (FileNotFoundError, PyPDF2.errors.PdfReadError, Exception) as e:\n",
        "        print(f\"Error reading {filepath}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "auYNPB5GTn1P"
      },
      "outputs": [],
      "source": [
        "def process_folder(folder_path):\n",
        "    # Process files in folder; use file extension to determine coherence labels.\n",
        "    all_data = []\n",
        "\n",
        "    # Check if folder exists\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"Error: {folder_path} is not a valid directory\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # process only supported file types\n",
        "        if filename.endswith((\".txt\", \".pdf\")):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            text = process_file(filepath)\n",
        "\n",
        "            if text and len(text.strip()) > 0:\n",
        "                # Only process if we have meaningful text\n",
        "                if filename.endswith(\".pdf\"):\n",
        "                  all_data.append({\"text\": text, \"label\": 1, \"filename\": filename})\n",
        "                elif filename.endswith(\".txt\"):\n",
        "                  all_data.append({\"text\": text, \"label\": 0, \"filename\": filename})\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"No valid files found or processed\")\n",
        "    # Print dataset statistics\n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "        coherent_count = sum(df['label'])\n",
        "        incoherent_count = len(df) - coherent_count\n",
        "        print(f\"Created dataset with {len(df)} examples ({coherent_count} coherent, {incoherent_count} incoherent)\")\n",
        "\n",
        "    return pd.DataFrame(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(df):\n",
        "    \"\"\"Extract features from text data.\"\"\"\n",
        "    # Get BERT embeddings\n",
        "    print(\"Extracting BERT embeddings...\")\n",
        "    embeddings = []\n",
        "    for text in tqdm(df['text'], desc=\"Processing documents\"):\n",
        "        embedding = get_bert_embeddings(text)\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    return np.array(embeddings)"
      ],
      "metadata": {
        "id": "M-H6xmnGs4_W"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Detailed evaluation of the model performance.\"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(y_pred == y_test)\n",
        "\n",
        "    # Get classification report\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    # Extract metrics more safely by checking all possible key formats\n",
        "    positive_class_keys = [1, '1', 1.0, '1.0']\n",
        "\n",
        "    # Find the first valid key or use default values\n",
        "    precision, recall, f1 = 0, 0, 0\n",
        "    for key in positive_class_keys:\n",
        "        if str(key) in report:\n",
        "            precision = report[str(key)]['precision']\n",
        "            recall = report[str(key)]['recall']\n",
        "            f1 = report[str(key)]['f1-score']\n",
        "            break\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Model Evaluation Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "QZY8AZpRsV_i"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(X, y):\n",
        "    \"\"\"Train and evaluate the coherence detection model.\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    model_coherence = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
        "    model_coherence.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"Coherence Model Evaluation:\")\n",
        "    evaluate_model(model_coherence, X_test, y_test)\n",
        "\n",
        "    return model_coherence, X_test, y_test"
      ],
      "metadata": {
        "id": "D6E8_67Vs-4_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_coherence(model, text):\n",
        "    \"\"\"Predict coherence for a new text.\"\"\"\n",
        "    # Get features\n",
        "    embedding = get_bert_embeddings(text)\n",
        "    embedding = embedding.reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(embedding)\n",
        "    confidence = model.predict_proba(embedding)[0][prediction[0]]\n",
        "\n",
        "    return prediction[0], confidence"
      ],
      "metadata": {
        "id": "Wi8gFWpJtCRK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_folder_path = '/content/drive/MyDrive/Research_Paper_Optimizer/datasets/QA_training_dataset'\n",
        "    test_folder_path = '/content/drive/MyDrive/Research_Paper_Optimizer/datasets/QA_test_dataset'\n",
        "\n",
        "    # Process training folder - PDFs are coherent, TXTs are incoherent\n",
        "    print(\"Processing training dataset...\")\n",
        "    train_df = process_folder(train_folder_path)\n",
        "\n",
        "    # Check if we have enough data\n",
        "    if len(train_df) < 4:  # Need at least two coherent and two incoherent examples\n",
        "        print(\"Not enough valid files found for training\")\n",
        "        exit()\n",
        "\n",
        "    # Extract features\n",
        "    X_train = extract_features(train_df)\n",
        "    y_train = train_df['label']\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model_coherence = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
        "    model_coherence.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Processing test dataset...\")\n",
        "    test_df = process_folder(test_folder_path)\n",
        "\n",
        "    if len(test_df) > 0:\n",
        "        # Extract features from test data\n",
        "        X_test = extract_features(test_df)\n",
        "        y_test = test_df['label']\n",
        "\n",
        "        # Evaluate model on test data\n",
        "        print(\"Evaluating model on test dataset:\")\n",
        "        evaluate_model(model_coherence, X_test, y_test)\n",
        "    else:\n",
        "        print(\"No test data found for evaluation\")\n",
        "\n",
        "    # Save the model to Google Drive\n",
        "    model_filename = \"/content/drive/MyDrive/Research_Paper_Optimizer/Quality_Assurance/coherence_model.joblib\"\n",
        "    joblib.dump(model_coherence, model_filename)\n",
        "    print(f\"Model saved to {model_filename}\")"
      ],
      "metadata": {
        "id": "Wlzh5vgmtIoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607a7e1f-9aa2-418e-e7b4-b748ff10795c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training dataset...\n",
            "Created dataset with 200 examples (100 coherent, 100 incoherent)\n",
            "Extracting BERT embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing documents: 100%|██████████| 200/200 [05:43<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Processing test dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-88976376') invalid; use 0.0 instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 49 examples (25 coherent, 24 incoherent)\n",
            "Extracting BERT embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing documents: 100%|██████████| 49/49 [01:26<00:00,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model on test dataset:\n",
            "Model Evaluation Results:\n",
            "Accuracy: 0.6327\n",
            "Precision: 0.6296\n",
            "Recall: 0.6800\n",
            "F1 Score: 0.6538\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.58      0.61        24\n",
            "           1       0.63      0.68      0.65        25\n",
            "\n",
            "    accuracy                           0.63        49\n",
            "   macro avg       0.63      0.63      0.63        49\n",
            "weighted avg       0.63      0.63      0.63        49\n",
            "\n",
            "Model saved to /content/drive/MyDrive/Research_Paper_Optimizer/Quality_Assurance/coherence_model.joblib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVfr5fXE1nSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}